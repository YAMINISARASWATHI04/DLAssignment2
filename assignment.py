# -*- coding: utf-8 -*-
"""Assignment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1efHyYAB5_i7KAKD7Q9tVj7-DdbAIlzpG
"""

# @title Importing Required Libraries
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, SimpleRNN, LSTM, GRU, Dense
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split

file_path = "/content/drive/MyDrive/Dakshinagiri/dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.train.tsv"

# @title Parsing the data
input_texts, target_texts = [], []
with open(file_path, 'r', encoding='utf-8') as f:
    for line in f:
        eng, mar = line.strip().split('\t')[:2]
        input_texts.append(eng)
        target_texts.append('\t' + mar + '\n')  # Add start/end tokens
print (input_texts)
print(target_texts)

# @title Tokenizing the data
input_chars = sorted(set("".join(input_texts)))
target_chars = sorted(set("".join(target_texts)))

input_token_index = {char: i+1 for i, char in enumerate(input_chars)}  # reserve 0 for padding
target_token_index = {char: i+1 for i, char in enumerate(target_chars)}
reverse_target_char_index = {i: char for char, i in target_token_index.items()}

input_vocab_size = len(input_token_index) + 1
target_vocab_size = len(target_token_index) + 1
print(input_chars)
print(target_chars)
print(input_token_index)
print(target_token_index)

# @title Format Dataset
encoder_input_data = [[input_token_index[c] for c in seq] for seq in input_texts]
decoder_input_data = [[target_token_index[c] for c in seq] for seq in target_texts]
decoder_target_data = [seq[1:] for seq in decoder_input_data]

max_encoder_seq_length = max(len(seq) for seq in encoder_input_data)
max_decoder_seq_length = max(len(seq) for seq in decoder_input_data)

encoder_input_data = pad_sequences(encoder_input_data, maxlen=max_encoder_seq_length, padding='post')
decoder_input_data = pad_sequences(decoder_input_data, maxlen=max_decoder_seq_length, padding='post')
decoder_target_data = pad_sequences(decoder_target_data, maxlen=max_decoder_seq_length, padding='post')

decoder_target_data_onehot = tf.keras.utils.to_categorical(decoder_target_data, num_classes=target_vocab_size)

def build_seq2seq_model(cell_type='LSTM', embedding_dim=64, hidden_dim=128, num_layers=1):
    RNN = {'RNN': SimpleRNN, 'LSTM': LSTM, 'GRU': GRU}[cell_type]

    # Encoder
    encoder_inputs = Input(shape=(None,))
    enc_embed = Embedding(input_vocab_size, embedding_dim)(encoder_inputs)

    for i in range(num_layers):
        return_seq = (i != num_layers - 1)
        if i == 0:
            encoder_output = RNN(hidden_dim, return_sequences=return_seq, return_state=True)
            if cell_type == 'LSTM':
                _, state_h, state_c = encoder_output(enc_embed)
                encoder_states = [state_h, state_c]
            else:
                _, state_h = encoder_output(enc_embed)
                encoder_states = [state_h]
        else:
            enc_embed = RNN(hidden_dim, return_sequences=return_seq)(enc_embed)

    # Decoder
    decoder_inputs = Input(shape=(None,))
    dec_embed = Embedding(target_vocab_size, embedding_dim)(decoder_inputs)

    for i in range(num_layers):
        return_seq = True
        if i == 0:
            decoder_rnn = RNN(hidden_dim, return_sequences=return_seq, return_state=True)
            if cell_type == 'LSTM':
                decoder_outputs, _, _ = decoder_rnn(dec_embed, initial_state=encoder_states)
            else:
                decoder_outputs, _ = decoder_rnn(dec_embed, initial_state=encoder_states)
        else:
            dec_embed = RNN(hidden_dim, return_sequences=return_seq)(dec_embed)

    decoder_dense = Dense(target_vocab_size, activation='softmax')
    decoder_outputs = decoder_dense(decoder_outputs)

    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)
    return model

model = build_seq2seq_model(cell_type='LSTM', embedding_dim=128, hidden_dim=256, num_layers=1)
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

model.fit(
    [encoder_input_data, decoder_input_data],
    decoder_target_data_onehot,
    batch_size=64,
    epochs=30,
    validation_split=0.2
)

from tensorflow import keras # Import keras from tensorflow

# Encoder inference model
encoder_inputs = model.input[0]  # Input to encoder
# Get the correct LSTM layer name - it's likely 'lstm_16' based on the error message
encoder_outputs, state_h_enc, state_c_enc = model.get_layer("lstm").output  # Changed 'lstm' to 'lstm_16'
encoder_states = [state_h_enc, state_c_enc]
encoder_model = keras.Model(encoder_inputs, encoder_states)

latent_dim = encoder_states[0].shape[-1]

decoder_inputs = model.input[1]  # Input to decoder
# Pass decoder inputs through embedding layer
dec_embed = model.get_layer("embedding_1")(decoder_inputs) # This line was added

decoder_state_input_h = keras.Input(shape=(latent_dim,), name="input_h")
decoder_state_input_c = keras.Input(shape=(latent_dim,), name="input_c")
decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]

# Similarly, adjust the decoder LSTM layer name - likely 'lstm_17'
decoder_lstm = model.get_layer("lstm_1")  # Changed 'lstm_1' to 'lstm_17'
decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(
    dec_embed, initial_state=decoder_states_inputs # decoder_inputs was changed to dec_embed
)
decoder_states = [state_h_dec, state_c_dec]
decoder_dense = model.get_layer("dense")  # Or "dense_18" if that's the actual name from the error
decoder_outputs = decoder_dense(decoder_outputs)

decoder_model = keras.Model(
    [decoder_inputs] + decoder_states_inputs,
    [decoder_outputs] + decoder_states
)

# @title Decoding Test Sentences
def decode_sequence(input_seq):
    # Encode the input as state vectors
    states_value = encoder_model.predict(input_seq)

    # Generate empty target sequence with just the start character
    target_seq = np.zeros((1, 1))
    target_seq[0, 0] = target_token_index['\t']

    # Store the decoded characters
    decoded_sentence = ""

    while True:
        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)

        # Sample a token
        sampled_token_index = np.argmax(output_tokens[0, -1, :])
        sampled_char = reverse_target_char_index[sampled_token_index]
        decoded_sentence += sampled_char

        # Exit condition: either hit max length or find stop character
        if (sampled_char == '\n' or len(decoded_sentence) > max_decoder_seq_length):
            break

        # Update the target sequence
        target_seq[0, 0] = sampled_token_index
        states_value = [h, c]

    return decoded_sentence

# @title Evaluating the Model
# Evaluate on test split
X_train, X_test, y_train, y_test = train_test_split(
    encoder_input_data, decoder_input_data, test_size=0.1, random_state=42)

# Predict sample
for i in range(5):
    input_seq = X_test[i:i+1]
    decoded = decode_sequence(input_seq)
    print("Input:", input_texts[i])
    print("Predicted:", decoded)

def load_test_data(filepath):
    test_pairs = []
    with open(filepath, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if '\t' in line:
                # Splitting the line by tabs and taking only the first two elements
                parts = line.split('\t')
                input_text, target_text = parts[:2]  # Assign the first two elements to input_text and target_text
                target_text = '\t' + target_text + '\n'  # for seq2seq start/end tokens
                test_pairs.append((input_text, target_text))
    return test_pairs
test_pairs = load_test_data('/content/drive/MyDrive/Dakshinagiri/dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.test.tsv')  # ‚úÖ update with your correct path
test_input_texts = [inp for inp, _ in test_pairs]
test_target_texts = [tgt for _, tgt in test_pairs]

embedding_dim = 64
hidden_dim = 256
cell_type = 'LSTM'
num_layers = 1

# Load test data

!pip install tensorflow
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical

tokenizer_input = Tokenizer()
tokenizer_input.fit_on_texts(input_texts)  # Fit on training data

tokenizer_output = Tokenizer(char_level=True)  # char_level=True for decoder
tokenizer_output.fit_on_texts(target_texts)

encoder_test_input_data = tokenizer_input.texts_to_sequences(test_input_texts)
encoder_test_input_data = pad_sequences(encoder_test_input_data, maxlen=max_encoder_seq_length, padding='post')

decoder_test_input_data = tokenizer_output.texts_to_sequences([t[:-1] for t in test_target_texts])
decoder_test_input_data = pad_sequences(decoder_test_input_data, maxlen=max_decoder_seq_length, padding='post')

decoder_test_target_data = tokenizer_output.texts_to_sequences([t[1:] for t in test_target_texts])
decoder_test_target_data = pad_sequences(decoder_test_target_data, maxlen=max_decoder_seq_length, padding='post')

# One-hot encode the targets
decoder_test_target_data_onehot = to_categorical(decoder_test_target_data, num_classes=target_vocab_size)

model = build_seq2seq_model(
    cell_type=cell_type,
    embedding_dim=embedding_dim,
    hidden_dim=hidden_dim,
    num_layers=num_layers
)

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

print(f"\nüîÅ Training model with embedding_dim={embedding_dim}, hidden_dim={hidden_dim}...")
model.fit(
    [encoder_input_data, decoder_input_data],
    decoder_target_data_onehot,
    batch_size=64,
    epochs=30,
    validation_split=0.2,
    verbose=1
)

print("\nüìä Evaluating on test set...")
test_loss, test_acc = model.evaluate(
    [encoder_test_input_data, decoder_test_input_data],
    decoder_test_target_data_onehot,
    verbose=1
)

print(f"\n‚úÖ Final Test Accuracy (embedding_dim={embedding_dim}): {test_acc:.4f}")

print("\nüîÆ Sample Predictions on Test Data:")
for i in range(10):  # Change this to len(test_input_texts) for all
    input_seq = encoder_test_input_data[i:i+1]
    decoded_sentence = decode_sequence(input_seq)

    print(f"{i+1}. Latin: {test_input_texts[i]}")
    print(f"   Predicted: {decoded_sentence}")
    print(f"   Actual:    {test_target_texts[i][1:-1]}")  # remove \t and \n
    print()